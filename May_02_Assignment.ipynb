{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b14e2465",
   "metadata": {},
   "source": [
    "### Q1. What is anomaly detection and what is its purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3c8926",
   "metadata": {},
   "source": [
    "Anomaly detection is a process of identifying data points, events, or observations that deviate from the normal behavior of a dataset ¹. It is used to detect unusual patterns or outliers in data that may indicate critical incidents such as technical glitches or potential opportunities such as changes in consumer behavior ³. Anomaly detection is widely used in various fields such as finance, healthcare, cybersecurity, and more ¹. \n",
    "\n",
    "The purpose of anomaly detection is to identify and flag unusual patterns or outliers in data that may require further investigation. It helps businesses to define system baselines, identify deviations from that baseline, and investigate inconsistent data ². Anomaly detection models are designed to proactively detect anomalies during the data in motion, which is of utmost importance when processing big data ¹. There are several anomaly detection models available such as Support Vector Machine (SVM), Isolation Forest, and Neural Networks ¹."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81009a57",
   "metadata": {},
   "source": [
    "### Q2. What are the key challenges in anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f1388d",
   "metadata": {},
   "source": [
    "There are several challenges in anomaly detection that can affect the accuracy of the model. Here are some of the key challenges:\n",
    "\n",
    "1. **Data quality**: The quality of the underlying dataset is a significant driver in creating an accurate and usable model. Data quality problems can include null data, incomplete datasets, inconsistent data formats, duplicate data, different scales of measurement, and human error ¹. To improve data quality, it's best to discard or fill null values, standardize all data formats before fitting the model, remove duplicate data based on time, preprocess input features into a standard scale before building a model, and reduce dependency on manually entered data by humans ¹.\n",
    "\n",
    "2. **Training sample sizes**: Having a large training set is essential for building an accurate representation of the expected value at a given time. If the training set is too small, then the algorithm doesn't have enough exposure to past examples to build an accurate representation of the expected value at a given time. Anomalies will skew the baseline, which will affect the overall accuracy of the model ¹.\n",
    "\n",
    "3. **Defining what is considered normal**: Defining what is considered normal can be challenging as it depends on various factors such as context, domain knowledge, and business requirements ³. It's important to define a universal hypothesis that encompasses all data and to consider different types of anomalies such as point anomaly, contextual anomaly, and group anomaly ².\n",
    "\n",
    "4. **Dealing with high dimensionality**: High dimensionality can make it difficult to extract useful features appropriately from data ³. It's important to use feature selection techniques such as Principal Component Analysis (PCA) or Linear Discriminant Analysis (LDA) to reduce dimensionality and improve model performance ².\n",
    "\n",
    "5. **Handling class imbalance**: Anomaly detection models often face class imbalance issues where there are significantly more normal values than anomalies ³. It's important to use techniques such as oversampling or undersampling to balance the classes and improve model performance ³."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0261d34f",
   "metadata": {},
   "source": [
    "### Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126706ca",
   "metadata": {},
   "source": [
    "In **supervised anomaly detection**, the algorithm is trained on a labeled dataset, where each data point is labeled as either normal or anomalous. The algorithm learns to identify anomalies based on the labeled data and can predict whether a new data point is normal or anomalous ²³. Supervised anomaly detection is useful when there is a clear distinction between normal and anomalous data points, and the labeled dataset is large enough to train the model ².\n",
    "\n",
    "In **unsupervised anomaly detection**, the algorithm is trained on an unlabeled dataset, where there are no predefined labels for normal or anomalous data points. The algorithm learns to identify anomalies based on the underlying distribution of the data points and can detect any deviation from that distribution ²³. Unsupervised anomaly detection is useful when there are no predefined labels for normal and anomalous data points, and the dataset is large enough to capture the underlying distribution of the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe30246",
   "metadata": {},
   "source": [
    "### Q4. What are the main categories of anomaly detection algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f2182c",
   "metadata": {},
   "source": [
    "There are three main categories of anomaly detection algorithms ¹²:\n",
    "\n",
    "1. **Supervised anomaly detection**: In this category, the algorithm is trained on a labeled dataset, where each data point is labeled as either normal or anomalous. The algorithm learns to identify anomalies based on the labeled data and can predict whether a new data point is normal or anomalous ²³. Supervised anomaly detection is useful when there is a clear distinction between normal and anomalous data points, and the labeled dataset is large enough to train the model ².\n",
    "\n",
    "2. **Unsupervised anomaly detection**: In this category, the algorithm is trained on an unlabeled dataset, where there are no predefined labels for normal or anomalous data points. The algorithm learns to identify anomalies based on the underlying distribution of the data points and can detect any deviation from that distribution ²³. Unsupervised anomaly detection is useful when there are no predefined labels for normal and anomalous data points, and the dataset is large enough to capture the underlying distribution of the data points ².\n",
    "\n",
    "3. **Semi-supervised anomaly detection**: In this category, the algorithm is trained on a partially labeled dataset, where only a small portion of the data points are labeled as normal or anomalous. The algorithm learns to identify anomalies based on both labeled and unlabeled data points ². Semi-supervised anomaly detection is useful when there are not enough labeled data points to train a supervised model, but there are some labeled data points available to guide the learning process ²."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c288c1ba",
   "metadata": {},
   "source": [
    "### Q5. What are the main assumptions made by distance-based anomaly detection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfde50bd",
   "metadata": {},
   "source": [
    "Distance-based anomaly detection methods rely on the assumption that normal data points are close to their neighbors, while anomalous data points are far from the normal data ². These methods use a distance or distance-derived metric between points and sets of points to identify anomalies ¹. The simplest anomaly detection algorithms are based on the assumptions about the data distribution, such as that data is one-dimensional and normally distributed. A large distance from the center of the distribution implies that the probability of observing such a data point is very small. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1415dd",
   "metadata": {},
   "source": [
    "### Q6. How does the LOF algorithm compute anomaly scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3941d9e",
   "metadata": {},
   "source": [
    "The **Local Outlier Factor (LOF)** algorithm is an unsupervised anomaly detection method that computes the local density deviation of a given data point with respect to its neighbors ¹. It considers as outliers the samples that have a substantially lower density than their neighbors ¹. The anomaly score values greater than 1.0 usually indicate the anomaly ².\n",
    "\n",
    "The LOF algorithm computes anomaly scores in the following way ²:\n",
    "\n",
    "1. For each data point, it identifies its k-nearest neighbors based on a distance metric such as Euclidean distance.\n",
    "2. It then computes the local reachability density (LRD) of each data point, which is the inverse of the average reachability distance of its k-nearest neighbors.\n",
    "3. The LOF of each data point is then computed as the ratio of the average LRD of its k-nearest neighbors to its own LRD.\n",
    "\n",
    "The LOF algorithm assigns a higher score to data points that have a lower density than their neighbors, indicating that they are more likely to be anomalous. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778b223c",
   "metadata": {},
   "source": [
    "### Q7. What are the key parameters of the Isolation Forest algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddaa16b",
   "metadata": {},
   "source": [
    "The **Isolation Forest** algorithm is an unsupervised anomaly detection method that uses binary trees to identify outliers in data ¹. Here are some of the key parameters of the Isolation Forest algorithm ²³:\n",
    "\n",
    "1. **n_estimators**: The number of base estimators in the ensemble. The default value is 100 ³.\n",
    "2. **max_samples**: The number of samples to draw from X to train each base estimator. The default value is \"auto,\" which means that the maximum number of samples is equal to the minimum of 256 and the number of samples provided ³.\n",
    "3. **contamination**: The proportion of outliers in the data set. The default value is \"auto,\" which means that the threshold is determined as in the original paper ³.\n",
    "4. **max_features**: The number of features to draw from X to train each base estimator. The default value is 1.0, which means that all features are used to train each base estimator ³.\n",
    "5. **bootstrap**: If True, individual trees are fit on random subsets of the training data sampled with replacement. If False, sampling without replacement is performed. The default value is False."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b029e6c",
   "metadata": {},
   "source": [
    "### Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score using KNN with K=10?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b898cf4",
   "metadata": {},
   "source": [
    "In K-Nearest Neighbors (KNN) anomaly detection, the anomaly score for a data point is often calculated based on the distance to its k-th nearest neighbor. In your case, you have K=10, so you would be looking at the distance to the 10th nearest neighbor.\n",
    "\n",
    "If a data point has only 2 neighbors of the same class within a radius of 0.5, this means it has 2 neighbors that are very close to it. However, the anomaly score is typically based on the distance to the k-th nearest neighbor, which in this case is 10th nearest neighbor.\n",
    "\n",
    "Assuming the distances to the 10th nearest neighbor are significantly larger (beyond the radius of 0.5), this data point would likely have a higher anomaly score because it is significantly different from its 10th nearest neighbor and, therefore, could be considered an anomaly.\n",
    "\n",
    "The exact calculation of the anomaly score can vary depending on the method and algorithm used for anomaly detection, so it's essential to refer to the specific formula or approach provided by the anomaly detection method you are using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b46a6b",
   "metadata": {},
   "source": [
    "### Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the anomaly score for a data point that has an average path length of 5.0 compared to the average path length of the trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636a6984",
   "metadata": {},
   "source": [
    "The **Isolation Forest** algorithm is an unsupervised anomaly detection method that uses binary trees to identify outliers in data . The anomaly score of a data point is calculated as the average path length of the data point in all trees . The anomaly score is normalized by dividing it by the expected path length of a random point from a uniform distribution . \n",
    "\n",
    "Given that the Isolation Forest algorithm has 100 trees and a dataset of 3000 data points, we can calculate the expected path length of a random point from a uniform distribution as follows:\n",
    "\n",
    "```\n",
    "c(n) = 2H(n-1) - (2(n-1)/n)\n",
    "```\n",
    "\n",
    "where `n` is the number of data points, and `H(i)` is the harmonic number defined as:\n",
    "\n",
    "```\n",
    "H(i) = 1 + 1/2 + 1/3 + ... + 1/i\n",
    "```\n",
    "\n",
    "For `n=3000`, we have:\n",
    "\n",
    "```\n",
    "c(3000) = 2H(2999) - (2(2999)/3000) = 11.089\n",
    "```\n",
    "\n",
    "Therefore, the expected path length of a random point from a uniform distribution is approximately **11.089** .\n",
    "\n",
    "The anomaly score for a data point with an average path length of 5.0 compared to the average path length of the trees can be calculated as follows:\n",
    "\n",
    "```\n",
    "anomaly_score = 2^(-5.0/11.089)\n",
    "```\n",
    "\n",
    "Therefore, the anomaly score for a data point with an average path length of 5.0 compared to the average path length of the trees is approximately **0.554** .\n",
    "\n",
    "I hope this helps! Let me know if you have any other questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
