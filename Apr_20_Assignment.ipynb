{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "775abc6a",
   "metadata": {},
   "source": [
    "### Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc98a88a",
   "metadata": {},
   "source": [
    "KNN stands for **K-Nearest Neighbors**. It is a **supervised learning algorithm** that can be used for both **classification** and **regression** problems. The algorithm works by finding the K nearest data points in the training set to the new data point and then classifying the new data point based on the majority class of its K nearest neighbors ¹²³. \n",
    "\n",
    "The algorithm is based on the assumption that similar things are close to each other. It is a **lazy learning algorithm**, which means that it does not learn a model from the training data but instead stores all of the training data and uses it to classify new data points at runtime ¹. \n",
    "\n",
    "The KNN algorithm can be used with different distance metrics such as Euclidean distance, Manhattan distance, and Minkowski distance ⁵. The working of the KNN algorithm can be explained using the following steps ¹:\n",
    "\n",
    "1. Select the number K of neighbors.\n",
    "2. Calculate the Euclidean distance of K number of neighbors.\n",
    "3. Take the K nearest neighbors as per the calculated Euclidean distance.\n",
    "4. Among these k neighbors, count the number of the data points in each category.\n",
    "5. Assign the new data points to that category for which the number of neighbors is maximum.\n",
    "\n",
    "KNN is a simple yet powerful algorithm that can be used for both classification and regression tasks ⁴."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f22a56",
   "metadata": {},
   "source": [
    "### Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319be018",
   "metadata": {},
   "source": [
    "The value of K in KNN is a **hyperparameter** that can be tuned to improve the performance of the algorithm. Choosing the right value of K is important because it can affect the accuracy of the model. There are different methods to choose the value of K, some of which are:\n",
    "\n",
    "1. **Elbow method**: In this method, we plot the accuracy of the model against different values of K. The value of K at which the accuracy starts to plateau is chosen as the optimal value of K ¹.\n",
    "2. **Cross-validation**: In this method, we split the data into training and validation sets and evaluate the performance of the model for different values of K. The value of K that gives the best performance on the validation set is chosen as the optimal value of K ².\n",
    "3. **Grid search**: In this method, we evaluate the performance of the model for different combinations of hyperparameters, including different values of K. The combination that gives the best performance is chosen as the optimal combination ³.\n",
    "\n",
    "It is important to note that choosing a very low value of K can lead to overfitting, while choosing a very high value can lead to underfitting ⁴. A commonly used value for K is 5 ³. Another simple approach to select K is to set `K=sqrt(n)`, where `n` is the number of data points in the training set ²."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0975ab",
   "metadata": {},
   "source": [
    "### Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff1259c",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) is a versatile algorithm that can be used for both classification and regression tasks. The main difference between the KNN classifier and KNN regressor lies in their objectives:\n",
    "\n",
    "1. **KNN Classifier:**\n",
    "   - Objective: The KNN classifier is used for classification tasks, where the goal is to categorize data points into predefined classes or categories.\n",
    "   - Output: It assigns a class label to each data point based on the majority class among its k-nearest neighbors.\n",
    "   - Use Case: Examples include image classification (e.g., recognizing handwritten digits), spam email detection, and sentiment analysis.\n",
    "\n",
    "2. **KNN Regressor:**\n",
    "   - Objective: The KNN regressor, on the other hand, is used for regression tasks, where the goal is to predict a continuous numerical value.\n",
    "   - Output: It predicts a numeric value for each data point based on the average (or weighted average) of the target values of its k-nearest neighbors.\n",
    "   - Use Case: Examples include predicting housing prices based on features like square footage and location or estimating a person's income based on their education and experience.\n",
    "\n",
    "In summary, KNN classifier is used for classification tasks, providing class labels, while KNN regressor is used for regression tasks, providing numerical predictions. Both algorithms rely on the distance between data points to make predictions, with the key difference being in the nature of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9bbb45",
   "metadata": {},
   "source": [
    "### Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352848e7",
   "metadata": {},
   "source": [
    "The performance of a K-Nearest Neighbors (KNN) model, whether for classification or regression, can be evaluated using various metrics. The choice of metric depends on the nature of the task (classification or regression) and the specific goals of the analysis. Here are common performance metrics for KNN:\n",
    "\n",
    "**For Classification:**\n",
    "\n",
    "1. **Accuracy:** This is the most basic metric for classification. It measures the proportion of correctly classified instances out of the total instances. It's suitable when classes are balanced.\n",
    "\n",
    "2. **Precision:** Precision measures the proportion of true positive predictions out of all positive predictions. It's useful when the cost of false positives is high.\n",
    "\n",
    "3. **Recall (Sensitivity):** Recall measures the proportion of true positive predictions out of all actual positives. It's useful when the cost of false negatives is high.\n",
    "\n",
    "4. **F1 Score:** The F1 Score is the harmonic mean of precision and recall. It provides a balance between precision and recall.\n",
    "\n",
    "5. **Confusion Matrix:** A confusion matrix provides a more detailed view of classification performance by showing true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "6. **ROC Curve (Receiver Operating Characteristic Curve):** ROC curves plot the true positive rate (TPR) against the false positive rate (FPR) at various thresholds, allowing you to assess the trade-off between sensitivity and specificity.\n",
    "\n",
    "7. **Area Under the ROC Curve (AUC-ROC):** AUC-ROC summarizes the performance of a classifier in a single value. Higher AUC values indicate better performance.\n",
    "\n",
    "8. **Precision-Recall Curve:** This curve plots precision against recall at various thresholds. It's useful when dealing with imbalanced datasets.\n",
    "\n",
    "**For Regression:**\n",
    "\n",
    "1. **Mean Absolute Error (MAE):** MAE measures the average absolute difference between the predicted and actual values. It is easy to interpret.\n",
    "\n",
    "2. **Mean Squared Error (MSE):** MSE measures the average squared difference between the predicted and actual values. It penalizes large errors more than MAE.\n",
    "\n",
    "3. **Root Mean Squared Error (RMSE):** RMSE is the square root of MSE and provides a measure of the error in the original units of the target variable.\n",
    "\n",
    "4. **R-squared (R²) or Coefficient of Determination:** R² measures the proportion of the variance in the target variable that is predictable from the independent variables. It ranges from 0 to 1, with higher values indicating better fit.\n",
    "\n",
    "5. **Mean Absolute Percentage Error (MAPE):** MAPE measures the average percentage difference between predicted and actual values. It is often used in business contexts.\n",
    "\n",
    "6. **Adjusted R-squared:** Adjusted R² adjusts R² for the number of predictors in the model, helping to account for model complexity.\n",
    "\n",
    "The choice of metric depends on the specific problem, dataset characteristics, and business objectives. It's often a good practice to evaluate a combination of these metrics to get a comprehensive view of model performance. Additionally, cross-validation is frequently used to assess a model's generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914b2a03",
   "metadata": {},
   "source": [
    "### Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71da4ad0",
   "metadata": {},
   "source": [
    "The \"curse of dimensionality\" is a term used in machine learning to describe the challenges and issues that arise as the dimensionality of the feature space (the number of features or dimensions) increases, particularly in the context of algorithms like K-Nearest Neighbors (KNN). It refers to the fact that as the number of dimensions or features grows, several problems and limitations become more pronounced. Here are some key aspects of the curse of dimensionality in KNN:\n",
    "\n",
    "1. **Increased Computational Complexity:** As the number of dimensions increases, the computational cost of distance calculations between data points becomes prohibitively high. This makes KNN computationally expensive, as it has to calculate distances between the query point and all data points in the dataset.\n",
    "\n",
    "2. **Data Sparsity:** In high-dimensional spaces, data points become sparse. This means that data points tend to be far apart from each other, making it more challenging to find meaningful nearest neighbors. The notion of \"closeness\" becomes less relevant as dimensionality increases.\n",
    "\n",
    "3. **Diminishing Discriminative Power:** In high-dimensional spaces, the differences between distances to the nearest and farthest neighbors tend to become small. This can result in less discriminative power for KNN, as it becomes challenging to distinguish between nearby and more distant neighbors.\n",
    "\n",
    "4. **Overfitting:** KNN can be prone to overfitting in high-dimensional spaces. With many dimensions, the model can easily fit noise in the data and generalize poorly to new data points.\n",
    "\n",
    "5. **Need for Feature Selection/Extraction:** In high-dimensional datasets, feature selection or dimensionality reduction techniques are often required to reduce the number of irrelevant or redundant features. This can help mitigate the curse of dimensionality.\n",
    "\n",
    "6. **Increased Sample Size Requirements:** To maintain the same level of statistical significance, the sample size required to estimate properties of the data (e.g., mean, variance) increases exponentially with dimensionality.\n",
    "\n",
    "7. **Distance Metric Sensitivity:** The choice of distance metric becomes critical in high-dimensional spaces. Some distance metrics may perform poorly when data is high-dimensional.\n",
    "\n",
    "To address the curse of dimensionality in KNN and other high-dimensional scenarios, practitioners often employ techniques such as dimensionality reduction (e.g., Principal Component Analysis), feature selection, or selecting an appropriate distance metric. Additionally, algorithms designed to handle high-dimensional data, like Locality-Sensitive Hashing (LSH), can be used to efficiently find approximate nearest neighbors in high-dimensional spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017bb7fa",
   "metadata": {},
   "source": [
    "### Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e3ddcc",
   "metadata": {},
   "source": [
    "To handle missing values in KNN, we can use the **KNNImputer** class from the **scikit-learn** library ¹. The KNNImputer is a robust way to impute missing values using the k-Nearest Neighbors Algorithm ². \n",
    "\n",
    "The KNN algorithm makes predictions based on a defined number of nearest neighbors. It calculates distances from an instance you want to classify to every other instance in the training set. We won’t use the algorithm for classification purposes but to fill missing values ¹. \n",
    "\n",
    "Here are the steps to handle missing values with KNN:\n",
    "\n",
    "1. Import the KNNImputer class from scikit-learn.\n",
    "2. Create an instance of the KNNImputer class with the desired number of neighbors.\n",
    "3. Call the fit_transform method on our imputer to impute missing data.\n",
    "\n",
    "Here's an example code snippet that demonstrates how to use KNNImputer to handle missing values:\n",
    "\n",
    "```python\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Create an instance of KNNImputer with 3 neighbors\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "\n",
    "# Impute missing data\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "```\n",
    "\n",
    "In this example, we have created an instance of KNNImputer with 3 neighbors and used it to impute missing data in our dataset `data`. You can adjust the number of neighbors as per your requirements ¹."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95a1a5e",
   "metadata": {},
   "source": [
    "### Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774645a4",
   "metadata": {},
   "source": [
    "The KNN algorithm can be used for both classification and regression tasks. The key difference between the two is the type of output variable they predict ¹²³.\n",
    "\n",
    "In classification, the KNN algorithm predicts the class to which the output variable belongs by computing the local probability. The KNN classifier predicts a class by using the highest majority category among its k nearest neighbors ¹³. \n",
    "\n",
    "In regression, the KNN algorithm predicts the value of the output variable by using a local average. The KNN regressor predicts a value by using the mean of the k nearest neighbors ¹³.\n",
    "\n",
    "The performance of KNN classifier and regressor depends on various factors such as the size of the dataset, number of features, and choice of distance metric. In general, KNN classifier performs better when there are fewer classes and more samples per class. On the other hand, KNN regressor performs better when there is a strong correlation between input and output variables ²."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b081c5d4",
   "metadata": {},
   "source": [
    "### Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cf749c",
   "metadata": {},
   "source": [
    "The KNN algorithm is a simple and intuitive machine learning algorithm that can be used for both classification and regression tasks ¹²³. Here are some of its strengths and weaknesses:\n",
    "\n",
    "**Strengths:**\n",
    "- KNN is easy to understand and implement.\n",
    "- It can work well with both binary and multi-class classification problems.\n",
    "- It can handle both linear and nonlinear decision boundaries.\n",
    "- It can be used for both regression and classification predictive problems ³⁴.\n",
    "\n",
    "**Weaknesses:**\n",
    "- KNN can be computationally expensive, especially when dealing with large datasets.\n",
    "- It is sensitive to the choice of distance metric, which can significantly impact the performance of the algorithm.\n",
    "- KNN does not work well with high-dimensional data because the distance between the nearest neighbors tends to converge to a constant value, making the algorithm less effective ¹²³.\n",
    "\n",
    "To address these weaknesses, researchers have proposed several modifications to the KNN algorithm. For example, using **KD-trees** or **ball trees** to speed up the search for nearest neighbors, or using **distance-weighted voting** to give more weight to closer neighbors. Another approach is to use **feature selection** or **dimensionality reduction** techniques to reduce the number of features in high-dimensional datasets ¹⁵."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6349d8fa",
   "metadata": {},
   "source": [
    "### Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f170c1",
   "metadata": {},
   "source": [
    "In KNN, Euclidean distance and Manhattan distance are two commonly used distance metrics to measure the similarity between two data points ¹². \n",
    "\n",
    "Euclidean distance is the straight-line distance between two points in a Euclidean space. It is calculated as the square root of the sum of the squared differences between the coordinates of two points ¹². \n",
    "\n",
    "Manhattan distance, also known as taxicab distance, is the sum of the absolute differences between the coordinates of two points. It is called taxicab distance because it measures the distance a taxi would have to travel along a grid of streets to reach its destination ¹².\n",
    "\n",
    "The main difference between Euclidean and Manhattan distances is that Euclidean distance is sensitive to both magnitude and direction of the differences between two points, while Manhattan distance is only sensitive to magnitude. This means that Euclidean distance gives more weight to large differences, while Manhattan distance gives equal weight to all differences ¹².\n",
    "\n",
    "In KNN, the choice of distance metric depends on the nature of the data and the problem at hand. Euclidean distance is commonly used when the data is continuous and has a small number of dimensions. Manhattan distance, on the other hand, is preferred when dealing with high-dimensional data or when outliers are present in the data ¹³."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b51c68",
   "metadata": {},
   "source": [
    "### Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4130b79b",
   "metadata": {},
   "source": [
    "Feature scaling is an important preprocessing step for many machine learning algorithms, including KNN ¹²³. The reason for scaling the features is to ensure that all features contribute equally to the distance metric used in KNN. If the features are not scaled, then the features with larger magnitudes will dominate the distance calculations, leading to biased results ¹².\n",
    "\n",
    "There are several methods for feature scaling, including **min-max scaling** and **standardization**. Min-max scaling scales the features to a fixed range (usually between 0 and 1), while standardization scales the features to have zero mean and unit variance ⁴. \n",
    "\n",
    "Here's an example code snippet that demonstrates how to use standardization to scale the features in a dataset:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create an instance of StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the features\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "```\n",
    "\n",
    "In this example, we have created an instance of `StandardScaler` and used it to scale the features in our dataset `data`. You can use other scaling techniques as well, depending on your requirements ⁴."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
