{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "443eea66",
   "metadata": {},
   "source": [
    "### Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28abe96b",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression, often simply referred to as Gradient Boosting, is a machine learning technique used for both regression and classification tasks. It's a part of the ensemble learning methods and is particularly popular for building powerful predictive models.\n",
    "\n",
    "Here's an overview of Gradient Boosting Regression:\n",
    "\n",
    "**1. Ensemble Learning:** Gradient Boosting is an ensemble learning method, which means it combines the predictions of multiple machine learning models (typically decision trees) to make more accurate predictions than any individual model. \n",
    "\n",
    "**2. Sequential Improvement:** Unlike bagging techniques like Random Forest, which build multiple models independently and combine their results, Gradient Boosting builds models sequentially. Each model is trained to correct the errors made by the previous models.\n",
    "\n",
    "**3. Gradient Descent:** The \"Gradient\" in Gradient Boosting comes from the fact that it uses gradient descent optimization to minimize a loss function. In the context of regression, Gradient Boosting minimizes the Mean Squared Error (MSE) loss.\n",
    "\n",
    "**4. Weak Learners:** Each model in the ensemble is typically a weak learner, such as a shallow decision tree with only a few levels (often called a \"stump\"). These weak learners are referred to as \"base learners\" or \"base models.\"\n",
    "\n",
    "**5. Weighted Updates:** During each iteration, the model assigns weights to data points based on the errors made by the previous model. Data points that were incorrectly predicted by the previous model are assigned higher weights, which means the next model will focus more on correcting these mistakes.\n",
    "\n",
    "**6. Combining Predictions:** The final prediction is made by combining the predictions of all the base models. Typically, a weighted sum is used, where models that performed better are given more weight.\n",
    "\n",
    "**7. Tuning Hyperparameters:** Gradient Boosting has several hyperparameters to fine-tune, such as the learning rate, the number of trees (or iterations), the depth of trees, and more.\n",
    "\n",
    "**8. Robustness:** Gradient Boosting is known for its robustness and ability to capture complex relationships in data. It can handle noisy data and outliers to some extent.\n",
    "\n",
    "Overall, Gradient Boosting Regression is a powerful technique for regression tasks, and it has variations like Gradient Boosting for classification and LightGBM, XGBoost, and CatBoost, which are optimized implementations of gradient boosting algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5093e7c9",
   "metadata": {},
   "source": [
    "### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9366507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as  np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eddba5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_features=4, n_informative=2,\n",
    "                       random_state=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9f8d041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cee6de9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0736dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00179f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d575236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07e12d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.18929014482433\n",
      "0.9379167338315\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(y_test, y_pred))\n",
    "print(r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da143a53",
   "metadata": {},
   "source": [
    "### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dba37b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = {\n",
    "    'learning_rate':[0.001, 0.01, 0.1, .2],\n",
    "    'n_estimators':[50, 100, 150, 200],\n",
    "    'max_depth':[2,3,4,5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f724bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5509a4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "[CV 1/5] END learning_rate=0.001, max_depth=2, n_estimators=50;, score=0.003 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.001, max_depth=2, n_estimators=50;, score=-0.165 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.001, max_depth=2, n_estimators=50;, score=0.015 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.001, max_depth=2, n_estimators=50;, score=0.069 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.001, max_depth=2, n_estimators=50;, score=-0.039 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.001, max_depth=2, n_estimators=100;, score=0.059 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.001, max_depth=2, n_estimators=100;, score=-0.114 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.001, max_depth=2, n_estimators=100;, score=0.042 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.001, max_depth=2, n_estimators=100;, score=0.130 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.001, max_depth=2, n_estimators=100;, score=0.025 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.001, max_depth=2, n_estimators=150;, score=0.112 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.001, max_depth=2, n_estimators=150;, score=-0.066 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.001, max_depth=2, n_estimators=150;, score=0.076 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.001, max_depth=2, n_estimators=150;, score=0.184 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.001, max_depth=2, n_estimators=150;, score=0.086 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.001, max_depth=2, n_estimators=200;, score=0.160 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.001, max_depth=2, n_estimators=200;, score=-0.022 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.001, max_depth=2, n_estimators=200;, score=0.109 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.001, max_depth=2, n_estimators=200;, score=0.233 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.001, max_depth=2, n_estimators=200;, score=0.143 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.001, max_depth=3, n_estimators=50;, score=0.018 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.001, max_depth=3, n_estimators=50;, score=-0.144 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.001, max_depth=3, n_estimators=50;, score=0.038 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.001, max_depth=3, n_estimators=50;, score=0.089 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.001, max_depth=3, n_estimators=50;, score=-0.039 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.001, max_depth=3, n_estimators=100;, score=0.081 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.001, max_depth=3, n_estimators=100;, score=-0.081 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.001, max_depth=3, n_estimators=100;, score=0.085 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.001, max_depth=3, n_estimators=100;, score=0.166 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.001, max_depth=3, n_estimators=100;, score=0.046 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.001, max_depth=3, n_estimators=150;, score=0.139 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.001, max_depth=3, n_estimators=150;, score=-0.023 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.001, max_depth=3, n_estimators=150;, score=0.130 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.001, max_depth=3, n_estimators=150;, score=0.234 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.001, max_depth=3, n_estimators=150;, score=0.121 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.001, max_depth=3, n_estimators=200;, score=0.193 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.001, max_depth=3, n_estimators=200;, score=0.030 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.001, max_depth=3, n_estimators=200;, score=0.171 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.001, max_depth=3, n_estimators=200;, score=0.295 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.001, max_depth=3, n_estimators=200;, score=0.187 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.001, max_depth=4, n_estimators=50;, score=0.032 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.001, max_depth=4, n_estimators=50;, score=-0.129 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.001, max_depth=4, n_estimators=50;, score=0.050 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.001, max_depth=4, n_estimators=50;, score=0.089 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.001, max_depth=4, n_estimators=50;, score=-0.019 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.001, max_depth=4, n_estimators=100;, score=0.104 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.001, max_depth=4, n_estimators=100;, score=-0.043 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.001, max_depth=4, n_estimators=100;, score=0.109 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.001, max_depth=4, n_estimators=100;, score=0.167 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.001, max_depth=4, n_estimators=100;, score=0.061 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.001, max_depth=4, n_estimators=150;, score=0.165 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.001, max_depth=4, n_estimators=150;, score=0.034 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.001, max_depth=4, n_estimators=150;, score=0.163 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.001, max_depth=4, n_estimators=150;, score=0.238 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.001, max_depth=4, n_estimators=150;, score=0.137 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.001, max_depth=4, n_estimators=200;, score=0.221 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.001, max_depth=4, n_estimators=200;, score=0.101 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.001, max_depth=4, n_estimators=200;, score=0.212 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.001, max_depth=4, n_estimators=200;, score=0.302 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.001, max_depth=4, n_estimators=200;, score=0.214 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.001, max_depth=5, n_estimators=50;, score=0.037 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.001, max_depth=5, n_estimators=50;, score=-0.128 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.001, max_depth=5, n_estimators=50;, score=0.052 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.001, max_depth=5, n_estimators=50;, score=0.090 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.001, max_depth=5, n_estimators=50;, score=-0.013 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.001, max_depth=5, n_estimators=100;, score=0.120 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.001, max_depth=5, n_estimators=100;, score=-0.041 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.001, max_depth=5, n_estimators=100;, score=0.115 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.001, max_depth=5, n_estimators=100;, score=0.170 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.001, max_depth=5, n_estimators=100;, score=0.077 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.001, max_depth=5, n_estimators=150;, score=0.199 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.001, max_depth=5, n_estimators=150;, score=0.037 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.001, max_depth=5, n_estimators=150;, score=0.172 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.001, max_depth=5, n_estimators=150;, score=0.243 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.001, max_depth=5, n_estimators=150;, score=0.155 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.001, max_depth=5, n_estimators=200;, score=0.265 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.001, max_depth=5, n_estimators=200;, score=0.108 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.001, max_depth=5, n_estimators=200;, score=0.224 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.001, max_depth=5, n_estimators=200;, score=0.308 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.001, max_depth=5, n_estimators=200;, score=0.238 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.01, max_depth=2, n_estimators=50;, score=0.376 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.01, max_depth=2, n_estimators=50;, score=0.195 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.01, max_depth=2, n_estimators=50;, score=0.344 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.01, max_depth=2, n_estimators=50;, score=0.469 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.01, max_depth=2, n_estimators=50;, score=0.362 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.01, max_depth=2, n_estimators=100;, score=0.569 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.01, max_depth=2, n_estimators=100;, score=0.490 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.01, max_depth=2, n_estimators=100;, score=0.606 total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END learning_rate=0.01, max_depth=2, n_estimators=100;, score=0.714 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.01, max_depth=2, n_estimators=100;, score=0.655 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.01, max_depth=2, n_estimators=150;, score=0.686 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.01, max_depth=2, n_estimators=150;, score=0.657 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.01, max_depth=2, n_estimators=150;, score=0.731 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.01, max_depth=2, n_estimators=150;, score=0.834 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.01, max_depth=2, n_estimators=150;, score=0.780 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.01, max_depth=2, n_estimators=200;, score=0.776 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.01, max_depth=2, n_estimators=200;, score=0.752 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.01, max_depth=2, n_estimators=200;, score=0.791 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.01, max_depth=2, n_estimators=200;, score=0.897 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.01, max_depth=2, n_estimators=200;, score=0.836 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.01, max_depth=3, n_estimators=50;, score=0.467 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.01, max_depth=3, n_estimators=50;, score=0.314 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.01, max_depth=3, n_estimators=50;, score=0.375 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.01, max_depth=3, n_estimators=50;, score=0.579 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.01, max_depth=3, n_estimators=50;, score=0.492 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.718 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.599 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.610 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.809 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.748 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.01, max_depth=3, n_estimators=150;, score=0.830 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.01, max_depth=3, n_estimators=150;, score=0.743 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.01, max_depth=3, n_estimators=150;, score=0.707 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.01, max_depth=3, n_estimators=150;, score=0.905 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.01, max_depth=3, n_estimators=150;, score=0.842 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.01, max_depth=3, n_estimators=200;, score=0.882 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.01, max_depth=3, n_estimators=200;, score=0.817 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.01, max_depth=3, n_estimators=200;, score=0.765 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.01, max_depth=3, n_estimators=200;, score=0.942 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.01, max_depth=3, n_estimators=200;, score=0.870 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.01, max_depth=4, n_estimators=50;, score=0.479 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.01, max_depth=4, n_estimators=50;, score=0.393 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.01, max_depth=4, n_estimators=50;, score=0.412 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.01, max_depth=4, n_estimators=50;, score=0.577 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.01, max_depth=4, n_estimators=50;, score=0.544 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.01, max_depth=4, n_estimators=100;, score=0.704 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.01, max_depth=4, n_estimators=100;, score=0.639 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.01, max_depth=4, n_estimators=100;, score=0.571 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.01, max_depth=4, n_estimators=100;, score=0.774 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.01, max_depth=4, n_estimators=100;, score=0.783 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.01, max_depth=4, n_estimators=150;, score=0.818 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.01, max_depth=4, n_estimators=150;, score=0.754 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.01, max_depth=4, n_estimators=150;, score=0.665 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.01, max_depth=4, n_estimators=150;, score=0.855 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.01, max_depth=4, n_estimators=150;, score=0.870 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.01, max_depth=4, n_estimators=200;, score=0.865 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.01, max_depth=4, n_estimators=200;, score=0.809 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.01, max_depth=4, n_estimators=200;, score=0.747 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.01, max_depth=4, n_estimators=200;, score=0.892 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.01, max_depth=4, n_estimators=200;, score=0.917 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.01, max_depth=5, n_estimators=50;, score=0.559 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.01, max_depth=5, n_estimators=50;, score=0.414 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.01, max_depth=5, n_estimators=50;, score=0.447 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.01, max_depth=5, n_estimators=50;, score=0.583 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.01, max_depth=5, n_estimators=50;, score=0.605 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.01, max_depth=5, n_estimators=100;, score=0.778 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.01, max_depth=5, n_estimators=100;, score=0.648 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.01, max_depth=5, n_estimators=100;, score=0.624 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.01, max_depth=5, n_estimators=100;, score=0.786 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.01, max_depth=5, n_estimators=100;, score=0.836 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.01, max_depth=5, n_estimators=150;, score=0.850 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.01, max_depth=5, n_estimators=150;, score=0.743 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.01, max_depth=5, n_estimators=150;, score=0.697 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.01, max_depth=5, n_estimators=150;, score=0.860 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.01, max_depth=5, n_estimators=150;, score=0.887 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.01, max_depth=5, n_estimators=200;, score=0.877 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.01, max_depth=5, n_estimators=200;, score=0.787 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.01, max_depth=5, n_estimators=200;, score=0.727 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.01, max_depth=5, n_estimators=200;, score=0.884 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.01, max_depth=5, n_estimators=200;, score=0.905 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=2, n_estimators=50;, score=0.903 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=2, n_estimators=50;, score=0.916 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=2, n_estimators=50;, score=0.893 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=2, n_estimators=50;, score=0.975 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=2, n_estimators=50;, score=0.864 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=2, n_estimators=100;, score=0.915 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=2, n_estimators=100;, score=0.943 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=2, n_estimators=100;, score=0.911 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=2, n_estimators=100;, score=0.983 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=2, n_estimators=100;, score=0.863 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=2, n_estimators=150;, score=0.916 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=2, n_estimators=150;, score=0.949 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=2, n_estimators=150;, score=0.914 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=2, n_estimators=150;, score=0.984 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=2, n_estimators=150;, score=0.879 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=2, n_estimators=200;, score=0.911 total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END learning_rate=0.1, max_depth=2, n_estimators=200;, score=0.951 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=2, n_estimators=200;, score=0.913 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=2, n_estimators=200;, score=0.985 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=2, n_estimators=200;, score=0.862 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=3, n_estimators=50;, score=0.934 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=3, n_estimators=50;, score=0.930 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=3, n_estimators=50;, score=0.848 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=3, n_estimators=50;, score=0.973 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=3, n_estimators=50;, score=0.844 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.947 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.938 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.848 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.975 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.850 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=3, n_estimators=150;, score=0.930 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=3, n_estimators=150;, score=0.940 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=3, n_estimators=150;, score=0.847 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=3, n_estimators=150;, score=0.975 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=3, n_estimators=150;, score=0.877 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=3, n_estimators=200;, score=0.947 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=3, n_estimators=200;, score=0.937 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=3, n_estimators=200;, score=0.847 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=3, n_estimators=200;, score=0.976 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=3, n_estimators=200;, score=0.841 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=4, n_estimators=50;, score=0.912 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=4, n_estimators=50;, score=0.868 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=4, n_estimators=50;, score=0.825 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=4, n_estimators=50;, score=0.914 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=4, n_estimators=50;, score=0.861 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=4, n_estimators=100;, score=0.880 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=4, n_estimators=100;, score=0.870 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=4, n_estimators=100;, score=0.827 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=4, n_estimators=100;, score=0.917 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=4, n_estimators=100;, score=0.904 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=4, n_estimators=150;, score=0.898 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=4, n_estimators=150;, score=0.871 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=4, n_estimators=150;, score=0.825 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=4, n_estimators=150;, score=0.913 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=4, n_estimators=150;, score=0.887 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=4, n_estimators=200;, score=0.895 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=4, n_estimators=200;, score=0.872 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=4, n_estimators=200;, score=0.828 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=4, n_estimators=200;, score=0.916 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=4, n_estimators=200;, score=0.857 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=5, n_estimators=50;, score=0.897 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=5, n_estimators=50;, score=0.828 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=5, n_estimators=50;, score=0.793 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=5, n_estimators=50;, score=0.894 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=5, n_estimators=50;, score=0.899 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=5, n_estimators=100;, score=0.871 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=5, n_estimators=100;, score=0.837 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=5, n_estimators=100;, score=0.796 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=5, n_estimators=100;, score=0.891 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=5, n_estimators=100;, score=0.857 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=5, n_estimators=150;, score=0.858 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=5, n_estimators=150;, score=0.834 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=5, n_estimators=150;, score=0.792 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=5, n_estimators=150;, score=0.897 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=5, n_estimators=150;, score=0.901 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=5, n_estimators=200;, score=0.870 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=5, n_estimators=200;, score=0.829 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=5, n_estimators=200;, score=0.796 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=5, n_estimators=200;, score=0.896 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=5, n_estimators=200;, score=0.926 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.2, max_depth=2, n_estimators=50;, score=0.911 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.2, max_depth=2, n_estimators=50;, score=0.935 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.2, max_depth=2, n_estimators=50;, score=0.937 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.2, max_depth=2, n_estimators=50;, score=0.983 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.2, max_depth=2, n_estimators=50;, score=0.882 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.2, max_depth=2, n_estimators=100;, score=0.912 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.2, max_depth=2, n_estimators=100;, score=0.938 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.2, max_depth=2, n_estimators=100;, score=0.935 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.2, max_depth=2, n_estimators=100;, score=0.983 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.2, max_depth=2, n_estimators=100;, score=0.899 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.2, max_depth=2, n_estimators=150;, score=0.911 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.2, max_depth=2, n_estimators=150;, score=0.940 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.2, max_depth=2, n_estimators=150;, score=0.935 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.2, max_depth=2, n_estimators=150;, score=0.982 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.2, max_depth=2, n_estimators=150;, score=0.898 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.2, max_depth=2, n_estimators=200;, score=0.911 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.2, max_depth=2, n_estimators=200;, score=0.940 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.2, max_depth=2, n_estimators=200;, score=0.935 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.2, max_depth=2, n_estimators=200;, score=0.983 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.2, max_depth=2, n_estimators=200;, score=0.897 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.2, max_depth=3, n_estimators=50;, score=0.924 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.2, max_depth=3, n_estimators=50;, score=0.918 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.2, max_depth=3, n_estimators=50;, score=0.870 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.2, max_depth=3, n_estimators=50;, score=0.977 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.2, max_depth=3, n_estimators=50;, score=0.847 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.2, max_depth=3, n_estimators=100;, score=0.942 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.2, max_depth=3, n_estimators=100;, score=0.914 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.2, max_depth=3, n_estimators=100;, score=0.868 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.2, max_depth=3, n_estimators=100;, score=0.978 total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END learning_rate=0.2, max_depth=3, n_estimators=100;, score=0.881 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.2, max_depth=3, n_estimators=150;, score=0.936 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.2, max_depth=3, n_estimators=150;, score=0.915 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.2, max_depth=3, n_estimators=150;, score=0.869 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.2, max_depth=3, n_estimators=150;, score=0.978 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.2, max_depth=3, n_estimators=150;, score=0.795 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.2, max_depth=3, n_estimators=200;, score=0.951 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.2, max_depth=3, n_estimators=200;, score=0.915 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.2, max_depth=3, n_estimators=200;, score=0.869 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.2, max_depth=3, n_estimators=200;, score=0.977 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.2, max_depth=3, n_estimators=200;, score=0.820 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.2, max_depth=4, n_estimators=50;, score=0.882 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.2, max_depth=4, n_estimators=50;, score=0.864 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.2, max_depth=4, n_estimators=50;, score=0.814 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.2, max_depth=4, n_estimators=50;, score=0.909 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.2, max_depth=4, n_estimators=50;, score=0.927 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.2, max_depth=4, n_estimators=100;, score=0.872 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.2, max_depth=4, n_estimators=100;, score=0.865 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.2, max_depth=4, n_estimators=100;, score=0.815 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.2, max_depth=4, n_estimators=100;, score=0.909 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.2, max_depth=4, n_estimators=100;, score=0.867 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.2, max_depth=4, n_estimators=150;, score=0.889 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.2, max_depth=4, n_estimators=150;, score=0.862 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.2, max_depth=4, n_estimators=150;, score=0.814 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.2, max_depth=4, n_estimators=150;, score=0.914 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.2, max_depth=4, n_estimators=150;, score=0.918 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.2, max_depth=4, n_estimators=200;, score=0.951 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.2, max_depth=4, n_estimators=200;, score=0.863 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.2, max_depth=4, n_estimators=200;, score=0.817 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.2, max_depth=4, n_estimators=200;, score=0.917 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.2, max_depth=4, n_estimators=200;, score=0.934 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.2, max_depth=5, n_estimators=50;, score=0.881 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.2, max_depth=5, n_estimators=50;, score=0.830 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.2, max_depth=5, n_estimators=50;, score=0.803 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.2, max_depth=5, n_estimators=50;, score=0.888 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.2, max_depth=5, n_estimators=50;, score=0.915 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.2, max_depth=5, n_estimators=100;, score=0.867 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.2, max_depth=5, n_estimators=100;, score=0.831 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.2, max_depth=5, n_estimators=100;, score=0.806 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.2, max_depth=5, n_estimators=100;, score=0.893 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.2, max_depth=5, n_estimators=100;, score=0.902 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.2, max_depth=5, n_estimators=150;, score=0.814 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.2, max_depth=5, n_estimators=150;, score=0.829 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.2, max_depth=5, n_estimators=150;, score=0.793 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.2, max_depth=5, n_estimators=150;, score=0.892 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.2, max_depth=5, n_estimators=150;, score=0.913 total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.2, max_depth=5, n_estimators=200;, score=0.867 total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.2, max_depth=5, n_estimators=200;, score=0.831 total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.2, max_depth=5, n_estimators=200;, score=0.804 total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.2, max_depth=5, n_estimators=200;, score=0.894 total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.2, max_depth=5, n_estimators=200;, score=0.878 total time=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n",
       "             param_grid={'learning_rate': [0.001, 0.01, 0.1, 0.2],\n",
       "                         'max_depth': [2, 3, 4, 5],\n",
       "                         'n_estimators': [50, 100, 150, 200]},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(GradientBoostingRegressor(), param_grid=parameter, cv=5, verbose=3)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bb4b8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.2, 'max_depth': 2, 'n_estimators': 100}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168449b6",
   "metadata": {},
   "source": [
    "### Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0594d68c",
   "metadata": {},
   "source": [
    "In Gradient Boosting, a weak learner refers to a base model or individual model that performs slightly better than random guessing but is not a highly accurate predictor on its own. These weak learners are typically simple models with limited predictive capacity. The concept of using weak learners is a key principle in ensemble learning methods like Gradient Boosting.\n",
    "\n",
    "Here are some characteristics of weak learners in the context of Gradient Boosting:\n",
    "\n",
    "1. **Simplicity:** Weak learners are intentionally kept simple to ensure that they have limited complexity. For example, in the context of decision trees, weak learners might be shallow trees with a small number of levels or nodes (often referred to as \"stumps\").\n",
    "\n",
    "2. **Low Predictive Power:** Weak learners have relatively low predictive power when considered individually. They might have a high error rate or make many incorrect predictions.\n",
    "\n",
    "3. **Independence:** Each weak learner is trained independently of the others. In the case of Gradient Boosting, the training process for one weak learner does not take into account the performance or predictions of the previous ones.\n",
    "\n",
    "4. **Sequential Improvement:** Despite their low individual performance, the key idea in Gradient Boosting is to combine these weak learners in a sequential manner. Each subsequent learner is trained to correct the errors made by the previous ones.\n",
    "\n",
    "5. **Complementary Weaknesses:** Ideally, weak learners should have complementary weaknesses. This means that one weak learner's errors should occur in areas where another weak learner excels. When combined, they compensate for each other's weaknesses and collectively improve predictive accuracy.\n",
    "\n",
    "6. **Ensemble of Weak Learners:** The strength of Gradient Boosting lies in its ability to create an ensemble of weak learners, where the collective decision of the ensemble becomes a strong predictor. By iteratively adding weak learners that focus on the mistakes of previous learners, Gradient Boosting builds a strong predictive model.\n",
    "\n",
    "The concept of weak learners is a fundamental aspect of ensemble methods like Gradient Boosting because it allows for the construction of highly accurate models by combining the strengths of multiple, individually weak models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e57fb6",
   "metadata": {},
   "source": [
    "### Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf332a31",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm can be summarized as follows:\n",
    "\n",
    "1. **Sequential Learning:** Gradient Boosting builds an ensemble of weak learners (usually decision trees) in a sequential manner. It starts with an initial weak learner and then adds subsequent weak learners to correct the errors made by the previous ones.\n",
    "\n",
    "2. **Focus on Errors:** At each step, the algorithm identifies the instances in the training data where the current ensemble of weak learners is making mistakes. It assigns higher weights to the misclassified instances, essentially focusing on the areas where the model is performing poorly.\n",
    "\n",
    "3. **Gradient Descent:** The term \"Gradient\" in Gradient Boosting comes from the fact that it uses gradient descent optimization to minimize the errors of the ensemble. It calculates the gradient of the loss function with respect to the ensemble's predictions and updates the ensemble in the direction that reduces this gradient.\n",
    "\n",
    "4. **Weighted Combination:** The algorithm assigns weights to each weak learner's predictions based on their performance. Weak learners that make fewer errors are given higher weights, indicating that their predictions are more trusted.\n",
    "\n",
    "5. **Boosting:** The name \"Boosting\" reflects the idea that each subsequent weak learner is boosted in its ability to correct the mistakes of the ensemble. It's like having a team of experts, where each new expert specializes in the areas where the previous experts struggle.\n",
    "\n",
    "6. **Aggregating Predictions:** Finally, Gradient Boosting aggregates the predictions of all weak learners, giving more weight to the predictions of the more accurate weak learners. The final prediction is the weighted sum of these individual predictions.\n",
    "\n",
    "7. **Strong Learner:** The result is a strong ensemble model that can achieve high predictive accuracy by iteratively refining its predictions and focusing on the most challenging instances in the data.\n",
    "\n",
    "In summary, Gradient Boosting is an ensemble learning technique that combines the predictions of multiple weak learners in a sequential manner, with a focus on correcting errors. It uses gradient descent optimization to adjust the ensemble's predictions and assign appropriate weights to each weak learner. This process continues until the ensemble achieves a high level of accuracy, making it a powerful machine learning algorithm for both regression and classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd71445",
   "metadata": {},
   "source": [
    "### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4688cbc",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential and adaptive manner. Here's a step-by-step explanation of how it constructs the ensemble:\n",
    "\n",
    "1. **Initialization:** Gradient Boosting starts with an initial weak learner, often a simple model like a decision tree with a small depth (a stump). This initial model provides the baseline prediction.\n",
    "\n",
    "2. **Calculate Residuals:** It then calculates the residuals or errors between the actual target values and the predictions made by the initial model. These residuals represent the mistakes or discrepancies that the model needs to correct.\n",
    "\n",
    "3. **Train Weak Learner:** The algorithm trains a new weak learner on the residuals generated in the previous step. This weak learner is designed to capture the patterns or relationships in the data that the initial model couldn't capture. Typically, this learner has limited depth and complexity to prevent overfitting.\n",
    "\n",
    "4. **Weighted Combination:** The predictions of the newly trained weak learner are combined with the predictions of the previous models, with each model's contribution weighted by its accuracy. Weak learners that perform better on the training data receive higher weights, indicating that their predictions are more influential.\n",
    "\n",
    "5. **Update Residuals:** The residuals are updated using the predictions of the newly trained weak learner. The residuals now represent the discrepancies that remain after considering the contributions of the previous models.\n",
    "\n",
    "6. **Iterate:** Steps 3 to 5 are repeated for a predefined number of iterations or until a stopping criterion is met. In each iteration, a new weak learner is trained on the updated residuals, and its predictions are combined with those of the previous models.\n",
    "\n",
    "7. **Final Ensemble:** The final ensemble model is the weighted sum of predictions from all the weak learners. The weights are determined based on each learner's performance on the training data.\n",
    "\n",
    "The key idea here is that each new weak learner is trained to correct the errors made by the ensemble of previous learners. By iteratively focusing on the most challenging instances (those with the largest residuals), Gradient Boosting gradually builds a strong ensemble model that can capture complex relationships in the data.\n",
    "\n",
    "This sequential and adaptive approach is what sets Gradient Boosting apart from other ensemble methods and allows it to achieve high predictive accuracy, making it a powerful machine learning technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5c216a",
   "metadata": {},
   "source": [
    "### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf11e1b",
   "metadata": {},
   "source": [
    "The mathematical intuition behind the Gradient Boosting algorithm involves the following steps:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Initialize the model with a simple prediction, often the mean of the target variable for regression or the most frequent class for classification.\n",
    "\n",
    "2. **Compute Residuals:**\n",
    "   - Calculate the residuals (errors) between the actual target values and the current predictions. These residuals represent the mistakes made by the current model.\n",
    "\n",
    "3. **Train a Weak Learner:**\n",
    "   - Train a weak learner (typically a decision tree with limited depth) on the residuals. The goal is to find a model that can predict the residuals, capturing the patterns that the current model couldn't.\n",
    "\n",
    "4. **Calculate Learning Rate:**\n",
    "   - Choose a learning rate (or shrinkage parameter) between 0 and 1. This parameter controls the step size at which the new model's predictions are added to the current predictions. Smaller values of the learning rate require more iterations but can lead to better generalization.\n",
    "\n",
    "5. **Update Predictions:**\n",
    "   - Multiply the predictions of the newly trained weak learner by the learning rate. This scaled prediction is then added to the current predictions. The learning rate controls the contribution of the new model to the ensemble.\n",
    "\n",
    "6. **Update Residuals:**\n",
    "   - Recalculate the residuals by subtracting the current predictions (including the contribution from the new model) from the actual target values. The residuals now represent the errors that need to be further reduced.\n",
    "\n",
    "7. **Iterate:**\n",
    "   - Repeat steps 3 to 6 for a predefined number of iterations or until a stopping criterion is met. In each iteration, a new weak learner is trained on the updated residuals, and its predictions are scaled and added to the current predictions.\n",
    "\n",
    "8. **Final Ensemble Model:**\n",
    "   - The final ensemble model is the sum of predictions from all the weak learners. The learning rate controls the weight of each learner's prediction in the ensemble.\n",
    "\n",
    "The intuition behind this process is that each new weak learner focuses on the errors (residuals) made by the current ensemble of models. By iteratively reducing these errors, the Gradient Boosting algorithm constructs a strong and accurate predictive model.\n",
    "\n",
    "The algorithm optimizes the combination of weak learners and their weights to minimize a loss function (e.g., mean squared error for regression or cross-entropy for classification) with respect to the residuals. This optimization process is often performed using gradient descent or a similar optimization technique, which is where the name \"Gradient Boosting\" originates.\n",
    "\n",
    "By gradually refining the predictions and learning from previous mistakes, Gradient Boosting can capture complex relationships in the data and achieve high predictive accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
